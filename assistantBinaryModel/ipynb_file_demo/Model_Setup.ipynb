{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1ZayU53bt4j"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "import math\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import gc\n",
        "from huggingface_hub import hf_hub_download\n",
        "from msclap import CLAP\n",
        "import re\n",
        "import tiktoken\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from IPython.display import display, HTML\n",
        "from IPython.display import Audio, display, HTML, clear_output\n",
        "import time\n",
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NormLayer(nn.Module):\n",
        "    def __init__(self, emb_dim, eps=1e-5):\n",
        "        super().__init__()\n",
        "        # These are learnable scales and shifts so the model can\n",
        "        # undo the normalization if it actually needs a different distribution.\n",
        "        self.gamma = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.bias = nn.Parameter(torch.zeros(emb_dim))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Traditional LayerNorm: we zero-center the data and scale it by variance.\n",
        "        # This keeps gradients from exploding or dying during deep training.\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, unbiased=False, keepdim=True)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        return self.gamma * norm_x + self.bias\n",
        "\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, emb_dim, eps=1e-8):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Llama-style normalization. It's faster because we skip the mean\n",
        "        # centering and only focus on the Root Mean Square.\n",
        "        # It's basically saying: \"just keep the scale consistent.\"\n",
        "        norm = x.norm(2, dim=-1, keepdim=True)  # L2 norm\n",
        "        rms = norm * (1.0 / x.size(-1))**0.5\n",
        "        return x / (rms + self.eps) * self.weight\n",
        "\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    def __init__(self, emb_dim: int, hidden_dim: int):\n",
        "        \"\"\"\n",
        "        SwiGLU is basically a 'gated' linear unit.\n",
        "        One side (W2) acts as a gate that decides what information\n",
        "        from the other side (W1) gets to pass through.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.W1 = nn.Linear(emb_dim, hidden_dim)\n",
        "        self.W2 = nn.Linear(emb_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # We split the input into two paths.\n",
        "        # v goes through SiLU (Swish) to create a non-linear mask.\n",
        "        u = self.W1(x)\n",
        "        v = self.W2(x)\n",
        "        # Element-wise multiplication: u is the signal, SiLU(v) is the gate.\n",
        "        return u * F.silu(v)\n",
        "\n",
        "\n",
        "class FFN(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "\n",
        "class SwiGLU_FFN(nn.Module):\n",
        "    def __init__(self, emb_dim: int, hidden_dim: int | None = None):\n",
        "        \"\"\"\n",
        "        The Feed-Forward Network (FFN) is the 'knowledge' center of the model.\n",
        "        We use SwiGLU here because it's more stable and expressive than standard ReLU,\n",
        "        which is why modern models like LLaMA and PaLM use it.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # If we don't specify a hidden dimension, we follow the 'magic' 8/3 rule.\n",
        "        if hidden_dim is None:\n",
        "            hidden_dim = int(8 * emb_dim / 3)\n",
        "\n",
        "        # SwiGLU handles the non-linear transformation\n",
        "        self.swiGLU = SwiGLU(emb_dim, hidden_dim)\n",
        "\n",
        "        # W_out maps the processed data back to our model's original embedding size\n",
        "        self.W_out = nn.Linear(hidden_dim, emb_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # First, expand and activate the features using SwiGLU\n",
        "        x = self.swiGLU(x)\n",
        "\n",
        "        # Then, project them back down to the model dimension\n",
        "        return self.W_out(x)"
      ],
      "metadata": {
        "id": "Uy2vD90xcq1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GQA_SWA_Flash(nn.Module):\n",
        "    \"\"\"\n",
        "    Grouped Query Attention (GQA) + Sliding Window Attention (SWA).\n",
        "\n",
        "    The goal here is speed. GQA saves memory by sharing keys/values across\n",
        "    multiple query heads, and SWA saves computation by making sure tokens\n",
        "    don't look too far back into the past.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 emb_dim,\n",
        "                 model_dim,\n",
        "                 max_context_len,\n",
        "                 drop_rate,\n",
        "                 heads_num,\n",
        "                 kv_groups=None,\n",
        "                 window_size=None,\n",
        "                 swa_size=None,\n",
        "                 qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.model_dim = model_dim\n",
        "        self.heads_num = heads_num                     # total query heads H\n",
        "\n",
        "        # Safety: ensure model_d is divisible by number of heads\n",
        "        assert model_dim % heads_num == 0, \"model_dim must be divisible by heads_num\"\n",
        "\n",
        "        self.head_dim = model_dim // heads_num           # dim per head\n",
        "        self.kv_groups = kv_groups or 1                # number of KV groups G\n",
        "        assert self.heads_num % self.kv_groups == 0, \\\n",
        "            \"heads_num must be divisible by kv_groups\"\n",
        "        self.q_in_group = self.heads_num // self.kv_groups  # H/G heads per KV group\n",
        "\n",
        "        # Linear projections for Query, Key, Value\n",
        "        self.W_q = nn.Linear(emb_dim, model_dim, bias=qkv_bias)\n",
        "        # In GQA, K/V have kv_groups groups of head_dim each: G * d_head\n",
        "        self.W_k = nn.Linear(emb_dim, self.head_dim * self.kv_groups, bias=qkv_bias)\n",
        "        self.W_v = nn.Linear(emb_dim, self.head_dim * self.kv_groups, bias=qkv_bias)\n",
        "\n",
        "        self.window_size = window_size or max_context_len   # for KV cache / inference\n",
        "        self.swa_size = swa_size or max_context_len         # for SWA during training\n",
        "        self.drop_rate = drop_rate\n",
        "\n",
        "        # --- KV cache (inference only) ---\n",
        "        # Will hold: (b, kv_groups, window_size, head_dim) in GQA\n",
        "        self.register_buffer(\"cache_k\", None, persistent=False)\n",
        "        self.register_buffer(\"cache_v\", None, persistent=False)\n",
        "\n",
        "        # Pointer into the ring buffer\n",
        "        self.cache_ptr = 0\n",
        "        self.cache_len = 0   # actual number of valid tokens in cache\n",
        "\n",
        "        self.final_proj = nn.Linear(model_dim, model_dim)\n",
        "\n",
        "    def reset_cache(self):\n",
        "        \"\"\"Call this before starting a new sequence.\"\"\"\n",
        "        self.cache_k = None\n",
        "        self.cache_v = None\n",
        "        self.cache_ptr = 0\n",
        "        self.cache_len = 0\n",
        "\n",
        "    def sliding_window_mask(self, seq_len, device):\n",
        "        \"\"\"\n",
        "        Build SWA mask of shape (T, T):\n",
        "        True  = masked\n",
        "        False = allowed\n",
        "        Token i can attend j if:\n",
        "          j <= i and (i - j) < swa_size\n",
        "        \"\"\"\n",
        "        i = torch.arange(seq_len, device=device).unsqueeze(1)  # (T, 1)\n",
        "        j = torch.arange(seq_len, device=device).unsqueeze(0)  # (1, T)\n",
        "        # Mask positions where j > i (future) or too far in the past (i - j >= swa_size)\n",
        "        return (j > i) | (i - j >= self.swa_size)\n",
        "\n",
        "    def forward(self, x, use_cache: bool = False, use_swa: bool = False):\n",
        "        \"\"\"\n",
        "        x:\n",
        "          - training / no-cache: (b, T, emb_d)\n",
        "          - inference / cache:\n",
        "                * first call after reset_cache (prefill): (b, T_prompt, emb_d)\n",
        "                * subsequent incremental calls:        (b, 1, emb_d)\n",
        "\n",
        "        Returns:\n",
        "            (b, T, model_d)\n",
        "        \"\"\"\n",
        "        b_size, seq_len, _ = x.shape\n",
        "\n",
        "        # ---- Q, K, V projections ----\n",
        "        Q = self.W_q(x)      # (b, T, H * d_head)\n",
        "        K_new = self.W_k(x)  # (b, T, G * d_head)\n",
        "        V_new = self.W_v(x)  # (b, T, G * d_head)\n",
        "\n",
        "        # ---- reshape Q into (b, H, T, d_head) ----\n",
        "        Q = Q.view(b_size, seq_len, self.heads_num, self.head_dim).transpose(1, 2)\n",
        "        # Q: (b, H, T_q, d_head)\n",
        "\n",
        "        # ---- reshape K,V into (b, G, T, d_head) ----\n",
        "        K_new = K_new.view(b_size, seq_len, self.kv_groups, self.head_dim).transpose(1, 2)\n",
        "        V_new = V_new.view(b_size, seq_len, self.kv_groups, self.head_dim).transpose(1, 2)\n",
        "        # K_new, V_new: (b, G, T_k_new, d_head)\n",
        "\n",
        "        if use_cache:\n",
        "            # ================= RING BUFFER KV-CACHE MODE =================\n",
        "\n",
        "            # Sliding window attention (use_swa) is a training-only feature, not used in cache mode\n",
        "            assert not use_swa, \"use_swa is not supported when use_cache=True\"\n",
        "\n",
        "            # Initialize cache buffers for this batch if needed\n",
        "            if self.cache_k is None or self.cache_k.size(0) != b_size:\n",
        "                # Fixed-size ring buffers for KV groups\n",
        "                # (b, G, window_size, d_head)\n",
        "                self.cache_k = torch.zeros(\n",
        "                    b_size, self.kv_groups, self.window_size, self.head_dim,\n",
        "                    device=x.device, dtype=K_new.dtype\n",
        "                )\n",
        "                self.cache_v = torch.zeros_like(self.cache_k)\n",
        "                self.cache_ptr = 0\n",
        "                self.cache_len = 0\n",
        "\n",
        "            if self.cache_len == 0:\n",
        "                # ---------- PREFILL: FIRST CACHED CALL (FULL PROMPT) ----------\n",
        "                # We process the entire prompt in one go and build the initial cache.\n",
        "                # For simplicity and correctness, require prompt length <= window_size.\n",
        "                assert seq_len <= self.window_size, \\\n",
        "                    \"In prefill (first cache) call, seq_len must be <= window_size\"\n",
        "\n",
        "                # Store full prompt K/V into the beginning of the ring buffer\n",
        "                # (no wrap-around since seq_len <= window_size and cache_ptr == 0)\n",
        "                insert_len = seq_len\n",
        "                end = insert_len\n",
        "                # Use K_new, V_new directly: (b, G, T, d)\n",
        "                self.cache_k[:, :, :end, :] = K_new\n",
        "                self.cache_v[:, :, :end, :] = V_new\n",
        "\n",
        "                self.cache_ptr = end % self.window_size\n",
        "                self.cache_len = insert_len\n",
        "\n",
        "                # For attention in prefill, just use K_new/V_new directly\n",
        "                K = K_new  # (b, G, T_k, d_head)\n",
        "                V = V_new  # (b, G, T_k, d_head)\n",
        "\n",
        "                # Q keeps shape (b, H, T_q, d_head), with T_q == seq_len\n",
        "                T_q = seq_len\n",
        "                T_k = seq_len  # same as cache_len here\n",
        "\n",
        "                # ---- expand K/V per query head within each group ----\n",
        "                # Q: (b, H, T_q, d), we view it as (b, G, H/G, T_q, d)\n",
        "                Qg = Q.reshape(b_size, self.kv_groups, self.q_in_group, T_q, self.head_dim)\n",
        "                # K, V: (b, G, T_k, d) -> (b, G, 1, T_k, d) then broadcast over q_in_group\n",
        "                Kg = K.unsqueeze(2)  # (b, G, 1, T_k, d)\n",
        "                Vg = V.unsqueeze(2)  # (b, G, 1, T_k, d)\n",
        "\n",
        "                # Broadcast to (b, G, q_in_group, T_k, d)\n",
        "                Kg = Kg.expand(b_size, self.kv_groups, self.q_in_group, T_k, self.head_dim)\n",
        "                Vg = Vg.expand(b_size, self.kv_groups, self.q_in_group, T_k, self.head_dim)\n",
        "\n",
        "                # Merge groups and heads back: (b, H, T_q, d), (b, H, T_k, d)\n",
        "                Q_flat = Qg.contiguous().view(b_size, self.heads_num, T_q, self.head_dim)\n",
        "                K_flat = Kg.contiguous().view(b_size, self.heads_num, T_k, self.head_dim)\n",
        "                V_flat = Vg.contiguous().view(b_size, self.heads_num, T_k, self.head_dim)\n",
        "\n",
        "                # Prefill: full causal attention over the prompt\n",
        "                att = F.scaled_dot_product_attention(\n",
        "                    Q_flat,      # (b, H, T_q, d)\n",
        "                    K_flat,      # (b, H, T_k, d)\n",
        "                    V_flat,      # (b, H, T_k, d)\n",
        "                    attn_mask=None,\n",
        "                    dropout_p=0.0,\n",
        "                    is_causal=True\n",
        "                )\n",
        "\n",
        "            else:\n",
        "                # ---------- INCREMENTAL DECODE: SUBSEQUENT CACHED CALLS ----------\n",
        "                # After prefill, we assume one-token-at-a-time decoding for strict causality.\n",
        "                assert seq_len == 1, \\\n",
        "                    \"After prefill, cache mode expects seq_len == 1 for incremental decoding\"\n",
        "\n",
        "                # Store new token(s) into ring buffer along time dim=2\n",
        "                insert_len = min(seq_len, self.window_size)\n",
        "                end = self.cache_ptr + insert_len\n",
        "                # Use only the last insert_len timesteps from K_new/V_new\n",
        "                K_slice = K_new[:, :, -insert_len:, :]  # (b, G, insert_len, d)\n",
        "                V_slice = V_new[:, :, -insert_len:, :]\n",
        "\n",
        "                if end <= self.window_size:\n",
        "                    # Straight write\n",
        "                    self.cache_k[:, :, self.cache_ptr:end, :] = K_slice\n",
        "                    self.cache_v[:, :, self.cache_ptr:end, :] = V_slice\n",
        "                else:\n",
        "                    # Wrap-around case: split write\n",
        "                    first = self.window_size - self.cache_ptr\n",
        "                    self.cache_k[:, :, self.cache_ptr:, :] = K_slice[:, :, :first, :]\n",
        "                    self.cache_k[:, :, :end - self.window_size, :] = K_slice[:, :, first:, :]\n",
        "\n",
        "                    self.cache_v[:, :, self.cache_ptr:, :] = V_slice[:, :, :first, :]\n",
        "                    self.cache_v[:, :, :end - self.window_size, :] = V_slice[:, :, first:, :]\n",
        "\n",
        "                self.cache_ptr = (self.cache_ptr + insert_len) % self.window_size\n",
        "                self.cache_len = min(self.cache_len + insert_len, self.window_size)\n",
        "\n",
        "                # Reconstruct ordered K/V: (b, G, T_k, d_head)\n",
        "                if self.cache_len < self.window_size:\n",
        "                    K = self.cache_k[:, :, :self.cache_len, :]\n",
        "                    V = self.cache_v[:, :, :self.cache_len, :]\n",
        "                else:\n",
        "                    K = torch.cat(\n",
        "                        (self.cache_k[:, :, self.cache_ptr:, :],\n",
        "                         self.cache_k[:, :, :self.cache_ptr, :]),\n",
        "                        dim=2\n",
        "                    )\n",
        "                    V = torch.cat(\n",
        "                        (self.cache_v[:, :, self.cache_ptr:, :],\n",
        "                         self.cache_v[:, :, :self.cache_ptr, :]),\n",
        "                        dim=2\n",
        "                    )\n",
        "                # K, V: (b, G, T_k, d_head)\n",
        "\n",
        "                T_q = seq_len              # 1\n",
        "                T_k = K.size(2)            # cache_len\n",
        "\n",
        "                # ---- expand K/V per query head within each group ----\n",
        "                # Q: (b, H, T_q, d), we view it as (b, G, H/G, T_q, d)\n",
        "                # Use reshape here to be safe with non-contiguous Q\n",
        "                Qg = Q.reshape(b_size, self.kv_groups, self.q_in_group, T_q, self.head_dim)\n",
        "                # K, V: (b, G, T_k, d) -> (b, G, 1, T_k, d) then broadcast over q_in_group\n",
        "                Kg = K.unsqueeze(2)  # (b, G, 1, T_k, d)\n",
        "                Vg = V.unsqueeze(2)  # (b, G, 1, T_k, d)\n",
        "\n",
        "                # Broadcast to (b, G, q_in_group, T_k, d)\n",
        "                Kg = Kg.expand(b_size, self.kv_groups, self.q_in_group, T_k, self.head_dim)\n",
        "                Vg = Vg.expand(b_size, self.kv_groups, self.q_in_group, T_k, self.head_dim)\n",
        "\n",
        "                # Merge groups and heads back: (b, H, T_q, d), (b, H, T_k, d)\n",
        "                Q_flat = Qg.contiguous().view(b_size, self.heads_num, T_q, self.head_dim)\n",
        "                K_flat = Kg.contiguous().view(b_size, self.heads_num, T_k, self.head_dim)\n",
        "                V_flat = Vg.contiguous().view(b_size, self.heads_num, T_k, self.head_dim)\n",
        "\n",
        "                # FlashAttention path (no explicit causal mask; cache only holds past + current token)\n",
        "                att = F.scaled_dot_product_attention(\n",
        "                    Q_flat,      # (b, H, T_q, d)\n",
        "                    K_flat,      # (b, H, T_k, d)\n",
        "                    V_flat,      # (b, H, T_k, d)\n",
        "                    attn_mask=None,\n",
        "                    dropout_p=0.0,\n",
        "                    is_causal=False\n",
        "                )\n",
        "\n",
        "        else:\n",
        "            # ================= NORMAL TRAINING MODE =================\n",
        "            # Full sequence attention with causal masking handled internally\n",
        "\n",
        "            # Here we don't need to actually store cache; just use K_new, V_new.\n",
        "            # K_new, V_new: (b, G, T, d)\n",
        "            # Expand them per query head similarly as above:\n",
        "\n",
        "            # Use reshape here as well to avoid view-on-transposed issues\n",
        "            Qg = Q.reshape(b_size, self.kv_groups, self.q_in_group, seq_len, self.head_dim)\n",
        "            Kg = K_new.unsqueeze(2)  # (b, G, 1, T, d)\n",
        "            Vg = V_new.unsqueeze(2)  # (b, G, 1, T, d)\n",
        "\n",
        "            Kg = Kg.expand(b_size, self.kv_groups, self.q_in_group, seq_len, self.head_dim)\n",
        "            Vg = Vg.expand(b_size, self.kv_groups, self.q_in_group, seq_len, self.head_dim)\n",
        "\n",
        "            Q_flat = Qg.contiguous().view(b_size, self.heads_num, seq_len, self.head_dim)\n",
        "            K_flat = Kg.contiguous().view(b_size, self.heads_num, seq_len, self.head_dim)\n",
        "            V_flat = Vg.contiguous().view(b_size, self.heads_num, seq_len, self.head_dim)\n",
        "\n",
        "            attn_mask = self.sliding_window_mask(seq_len, device=Q_flat.device) if use_swa else None\n",
        "\n",
        "            att = F.scaled_dot_product_attention(\n",
        "                Q_flat,      # (b, H, T, d)\n",
        "                K_flat,      # (b, H, T, d)\n",
        "                V_flat,      # (b, H, T, d)\n",
        "                attn_mask=attn_mask,\n",
        "                dropout_p=self.drop_rate if self.training else 0.0,\n",
        "                is_causal=False if use_swa else True\n",
        "            )\n",
        "\n",
        "        # Merge heads: (b, H, T, d) â†’ (b, T, H*d) = (b, T, model_d)\n",
        "        out = att.transpose(1, 2).reshape(b_size, seq_len, self.model_dim)\n",
        "        return self.final_proj(out)\n",
        "\n",
        "\n",
        "class HDynMoF(nn.Module):\n",
        "    \"\"\"\n",
        "    Adaptive hierarchical sparse MoE with:\n",
        "      - Group-level top-p routing (adaptive #groups per token)\n",
        "      - Expert-level top-p routing within each active group (adaptive #experts)\n",
        "      - True sparse execution: each expert sees only its routed tokens.\n",
        "\n",
        "    cfg keys:\n",
        "      emb_dim              : int, embedding dim\n",
        "      e_num                : int, total experts across all groups\n",
        "      moe_groups           : int, number of groups\n",
        "      moe_group_top_p      : float, group-level top-p threshold per token\n",
        "      moe_max_groups       : int, max groups per token\n",
        "      moe_top_p            : float, expert-level top-p threshold per token\n",
        "      moe_max_k            : int, max experts per group per token\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cfg: dict):\n",
        "        super().__init__()\n",
        "\n",
        "        emb_dim = int(cfg[\"emb_dim\"])\n",
        "\n",
        "        # ---- Core MoE config ----\n",
        "        self.e_num = int(cfg.get(\"e_num\", 16))\n",
        "        self.num_groups = int(cfg.get(\"moe_groups\", 4))\n",
        "\n",
        "        # Expert routing config\n",
        "        self.top_p = float(cfg.get(\"moe_top_p\", 0.9))\n",
        "        self.max_k_per_grp = int(cfg.get(\"moe_max_k\", 2))\n",
        "\n",
        "        # Group routing config\n",
        "        self.group_top_p = float(cfg.get(\"moe_group_top_p\", 0.9))\n",
        "        self.max_groups_per_token = int(cfg.get(\"moe_max_groups\", self.num_groups))\n",
        "\n",
        "        # ---- Sanity checks ----\n",
        "        assert self.e_num >= 1, \"e_num must be >= 1\"\n",
        "        assert self.num_groups >= 1, \"moe_groups must be >= 1\"\n",
        "        assert 0.0 < self.top_p <= 1.0, \"moe_top_p must be in (0, 1]\"\n",
        "        assert 0.0 < self.group_top_p <= 1.0, \"moe_group_top_p must be in (0, 1]\"\n",
        "        assert self.e_num % self.num_groups == 0, \"e_num must be divisible by moe_groups\"\n",
        "\n",
        "        self.exp_per_group = self.e_num // self.num_groups\n",
        "\n",
        "        assert 1 <= self.max_k_per_grp <= self.exp_per_group, \\\n",
        "            f\"moe_max_k must be in [1, {self.exp_per_group}], got {self.max_k_per_grp}\"\n",
        "\n",
        "        assert 1 <= self.max_groups_per_token <= self.num_groups, \\\n",
        "            f\"moe_max_groups must be in [1, {self.num_groups}], got {self.max_groups_per_token}\"\n",
        "\n",
        "        # ---- Experts ----\n",
        "        # experts[g][e] = expert e in group g\n",
        "        self.experts = nn.ModuleList([\n",
        "            nn.ModuleList([\n",
        "                SwiGLU_FFN(emb_dim, 4 * emb_dim)\n",
        "                for _ in range(self.exp_per_group)\n",
        "            ])\n",
        "            for _ in range(self.num_groups)\n",
        "        ])\n",
        "\n",
        "        # ---- Gating over experts (per group) ----\n",
        "        self.gates = nn.ModuleList([\n",
        "            nn.Linear(emb_dim, self.exp_per_group)\n",
        "            for _ in range(self.num_groups)\n",
        "        ])\n",
        "\n",
        "        # ---- Group router ----\n",
        "        self.group_router = nn.Linear(emb_dim, self.num_groups)\n",
        "\n",
        "        # Variance scaling across groups\n",
        "        self.group_scale = 1.0 / math.sqrt(self.num_groups)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x: (B, T, D)\n",
        "        returns: (B, T, D)\n",
        "        \"\"\"\n",
        "        B, T, D = x.shape\n",
        "        device = x.device\n",
        "        dtype = x.dtype\n",
        "\n",
        "        # Flatten tokens: N = B * T\n",
        "        x_flat = x.reshape(-1, D)              # (N, D)\n",
        "        N = x_flat.size(0)\n",
        "\n",
        "        # Output accumulator\n",
        "        out_flat = x_flat.new_zeros((N, D))    # (N, D)\n",
        "\n",
        "        # ============================================================\n",
        "        # 1) Group-level routing (vectorized)\n",
        "        # ============================================================\n",
        "        # (B, T, G)\n",
        "        group_logits = self.group_router(x)\n",
        "        group_probs = F.softmax(group_logits, dim=-1)\n",
        "\n",
        "        # sort along groups dim\n",
        "        group_sorted_probs, group_sorted_idx = group_probs.sort(\n",
        "            dim=-1, descending=True\n",
        "        )  # (B, T, G)\n",
        "\n",
        "        G = self.num_groups\n",
        "        Kg = min(self.max_groups_per_token, G)\n",
        "\n",
        "        # top-Kg candidates per token\n",
        "        group_probs_k = group_sorted_probs[..., :Kg]        # (B, T, Kg)\n",
        "        group_idx_k = group_sorted_idx[..., :Kg]            # (B, T, Kg)\n",
        "\n",
        "        # cumulative prob for top-p\n",
        "        group_cum = group_probs_k.cumsum(dim=-1)            # (B, T, Kg)\n",
        "        group_active_mask = (group_cum <= self.group_top_p) # (B, T, Kg)\n",
        "        group_active_mask[..., 0] = True                    # always keep best group\n",
        "\n",
        "        # flatten to (N, Kg)\n",
        "        group_probs_k_flat = group_probs_k.reshape(N, Kg)           # (N, Kg)\n",
        "        group_idx_k_flat = group_idx_k.reshape(N, Kg)               # (N, Kg)\n",
        "        group_active_flat = group_active_mask.reshape(N, Kg)        # (N, Kg)\n",
        "\n",
        "        # masked + renormalize\n",
        "        group_masked_probs = group_probs_k_flat * group_active_flat.to(dtype)\n",
        "        group_sum_active = group_masked_probs.sum(dim=-1, keepdim=True) + 1e-9\n",
        "        group_renorm_probs = group_masked_probs / group_sum_active             # (N, Kg)\n",
        "\n",
        "        # ============================================================\n",
        "        # 2) Group-wise expert routing (sparse, but GPU-friendly)\n",
        "        # ============================================================\n",
        "        # We keep the outer loop over groups (usually small: 4â€“8).\n",
        "        for g in range(self.num_groups):\n",
        "            gate_g = self.gates[g]\n",
        "            experts_g = self.experts[g]\n",
        "\n",
        "            # tokens & slots where group g is chosen\n",
        "            # mask_group: (N, Kg)\n",
        "            mask_group = (group_idx_k_flat == g) & group_active_flat\n",
        "            if not mask_group.any():\n",
        "                continue\n",
        "\n",
        "            # indices of active (token, slot) for this group\n",
        "            active_pos = mask_group.nonzero(as_tuple=False)    # (M_g, 2)\n",
        "            if active_pos.numel() == 0:\n",
        "                continue\n",
        "\n",
        "            token_idx_g = active_pos[:, 0]          # (M_g,)\n",
        "            group_slot_idx_g = active_pos[:, 1]     # (M_g,)\n",
        "\n",
        "            # tokens that use this group (with repetition if same token picked\n",
        "            # group g via multiple slots â€” rare, but we handle it)\n",
        "            x_g_flat = x_flat[token_idx_g]          # (M_g, D)\n",
        "            group_weight_g = group_renorm_probs[token_idx_g, group_slot_idx_g]  # (M_g,)\n",
        "\n",
        "            M_g = x_g_flat.size(0)\n",
        "            if M_g == 0:\n",
        "                continue\n",
        "\n",
        "            # --------------------------------------------------------\n",
        "            # Expert-level routing for group g (vectorized over tokens)\n",
        "            # --------------------------------------------------------\n",
        "            # (M_g, E_g)\n",
        "            gate_logits_g = gate_g(x_g_flat)\n",
        "            probs_g = F.softmax(gate_logits_g, dim=-1)\n",
        "\n",
        "            # sort experts by prob\n",
        "            sorted_probs, sorted_idx = probs_g.sort(dim=-1, descending=True)  # (M_g, E_g)\n",
        "\n",
        "            K = min(self.max_k_per_grp, self.exp_per_group)\n",
        "            sorted_probs_k = sorted_probs[:, :K]      # (M_g, K)\n",
        "            sorted_idx_k = sorted_idx[:, :K]          # (M_g, K)\n",
        "\n",
        "            # top-p within group\n",
        "            cum_probs = sorted_probs_k.cumsum(dim=-1)         # (M_g, K)\n",
        "            active_mask = (cum_probs <= self.top_p)           # (M_g, K)\n",
        "            active_mask[:, 0] = True                          # always keep best\n",
        "\n",
        "            # renormalize only over active experts\n",
        "            masked_probs = sorted_probs_k * active_mask.to(dtype)     # (M_g, K)\n",
        "            sum_active = masked_probs.sum(dim=-1, keepdim=True) + 1e-9\n",
        "            renorm_probs = masked_probs / sum_active                  # (M_g, K)\n",
        "\n",
        "            # --------------------------------------------------------\n",
        "            # Combine group + expert routing into a single mapping:\n",
        "            #   for each active (token_local, expert_local)\n",
        "            #   we know: global_token_idx, expert_local_idx, weight\n",
        "            # --------------------------------------------------------\n",
        "            active_exp_pos = active_mask.nonzero(as_tuple=False)          # (M_ge, 2)\n",
        "            if active_exp_pos.numel() == 0:\n",
        "                continue\n",
        "\n",
        "            token_local_all = active_exp_pos[:, 0]    # (M_ge,)\n",
        "            slot_all = active_exp_pos[:, 1]           # (M_ge,)\n",
        "\n",
        "            # Map local token indices back to global [0, N)\n",
        "            global_token_all = token_idx_g[token_local_all]               # (M_ge,)\n",
        "\n",
        "            # Which local expert each (token, slot) chose\n",
        "            expert_local_all = sorted_idx_k[token_local_all, slot_all]    # (M_ge,)\n",
        "\n",
        "            # expert-level probs p_{g,e}(x)\n",
        "            p_all = renorm_probs[token_local_all, slot_all]               # (M_ge,)\n",
        "            # group-level probs q_g(x)\n",
        "            q_all = group_weight_g[token_local_all]                       # (M_ge,)\n",
        "\n",
        "            # Total mixture weights\n",
        "            total_weight_all = (q_all * p_all).to(dtype)                  # (M_ge,)\n",
        "\n",
        "            # --------------------------------------------------------\n",
        "            # Sparse expert execution:\n",
        "            #   we still loop over experts in this group,\n",
        "            #   but everything else is precomputed and vectorized.\n",
        "            # --------------------------------------------------------\n",
        "            # To avoid repeated comparisons, we can pre-sort by expert id\n",
        "            # and then process experts by slices.\n",
        "            # (Optional but nice: reduces number of boolean masks.)\n",
        "            sort_by_expert = torch.argsort(expert_local_all)\n",
        "            expert_local_sorted = expert_local_all[sort_by_expert]\n",
        "            global_token_sorted = global_token_all[sort_by_expert]\n",
        "            weight_sorted = total_weight_all[sort_by_expert]\n",
        "\n",
        "            # Find boundaries where expert id changes\n",
        "            # unique_experts: (U,)\n",
        "            # counts: (U,)\n",
        "            unique_experts, counts = torch.unique_consecutive(\n",
        "                expert_local_sorted, return_counts=True\n",
        "            )\n",
        "\n",
        "            # prefix sums for slicing\n",
        "            offsets = counts.cumsum(dim=0)\n",
        "            starts = torch.cat([\n",
        "                offsets.new_zeros((1,)),\n",
        "                offsets[:-1]\n",
        "            ], dim=0)  # (U,)\n",
        "\n",
        "            # iterate only over actually used experts (unique_experts)\n",
        "            for idx_u, e_local in enumerate(unique_experts.tolist()):\n",
        "                start = int(starts[idx_u].item())\n",
        "                end = int(offsets[idx_u].item())\n",
        "                if end <= start:\n",
        "                    continue\n",
        "\n",
        "                # slice for this expert\n",
        "                token_slice = global_token_sorted[start:end]   # (M_e,)\n",
        "                w_slice = weight_sorted[start:end]             # (M_e,)\n",
        "\n",
        "                x_e = x_flat[token_slice]                      # (M_e, D)\n",
        "                y_e = experts_g[e_local](x_e)                  # (M_e, D)\n",
        "\n",
        "                w_e = (w_slice * self.group_scale).unsqueeze(-1)  # (M_e, 1)\n",
        "                out_flat[token_slice] += w_e * y_e\n",
        "\n",
        "        # back to (B, T, D)\n",
        "        out = out_flat.reshape(B, T, D)\n",
        "        return out"
      ],
      "metadata": {
        "id": "yQCXpL79c1Ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock_GQA_SWA(nn.Module):\n",
        "    \"\"\"\n",
        "    The TransformerBlock is the fundamental computational unit of the model.\n",
        "    It is designed to be highly flexible, supporting three distinct 'wiring' modes:\n",
        "\n",
        "    1. Classic Mode: Sequential processing where attention is followed by the FFN.\n",
        "    2. Parallel Mode: Attention and FFN run simultaneously for faster computation.\n",
        "    3. Dual-Stream Mode: A custom experimental architecture that separates global\n",
        "       context (Attention) from local expertise (MoE) into two parallel paths.\n",
        "    \"\"\"\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "\n",
        "        self.use_parallel_att = cfg.get(\"use_parallel_att\", False)\n",
        "        self.use_RMSNorm      = cfg.get(\"use_RMSNorm\", False)\n",
        "        self.use_adaptive_moe = cfg.get(\"use_adaptive_moe\", False)\n",
        "\n",
        "        # NEW: dual global/local stream mode (push global/local all the way)\n",
        "        # When True, we keep separate global (attn) and local (MoE) streams\n",
        "        # through all layers and only combine at the very end of the model.\n",
        "        self.use_dual_stream  = cfg.get(\"use_dual_stream\", False)\n",
        "\n",
        "        self.att = GQA_SWA_Flash(\n",
        "            emb_dim=cfg[\"emb_dim\"],\n",
        "            model_dim=cfg[\"emb_dim\"],\n",
        "            max_context_len=cfg[\"context_length\"],\n",
        "            drop_rate=cfg[\"drop_rate\"],\n",
        "            heads_num=cfg[\"n_heads\"],\n",
        "            kv_groups=cfg[\"kv_groups\"],\n",
        "            swa_size=cfg[\"swa_size\"],\n",
        "            qkv_bias=cfg[\"qkv_bias\"],\n",
        "        )\n",
        "\n",
        "        # --- MoE depend on mode ---\n",
        "        MoE_FFN = HDynMoF if self.use_adaptive_moe else FFN\n",
        "        self.ff = MoE_FFN(cfg)\n",
        "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        # --- Norms depend on mode ---\n",
        "        Normalization = RMSNorm if self.use_RMSNorm else NormLayer\n",
        "\n",
        "        if self.use_dual_stream:\n",
        "            # Dual-stream: separate norms for global and local paths\n",
        "            #  - global stream goes through attention only\n",
        "            #  - local stream goes through MoE/FFN only\n",
        "            self.ln_global = Normalization(cfg[\"emb_dim\"])\n",
        "            self.ln_local  = Normalization(cfg[\"emb_dim\"])\n",
        "            # We keep a âˆš2 scale handy if we ever want to scale inside the block\n",
        "            self.res_scale = 1.0 / math.sqrt(2.0)\n",
        "            self.enhance_global_ctx = nn.GELU()\n",
        "\n",
        "        elif self.use_parallel_att:\n",
        "            # Single norm for both att and ff paths (parallel residual)\n",
        "            self.ln = Normalization(cfg[\"emb_dim\"])\n",
        "            self.res_scale = 1.0 / math.sqrt(2.0)\n",
        "        else:\n",
        "            # Classic pre-norm: separate ln1/ln2\n",
        "            self.ln1 = Normalization(cfg[\"emb_dim\"])\n",
        "            self.ln2 = Normalization(cfg[\"emb_dim\"])\n",
        "\n",
        "    def forward(self, x, use_cache: bool = False, use_swa: bool = False):\n",
        "        if self.use_dual_stream:\n",
        "            # Dual-stream mode:\n",
        "            # x is a tuple: (x_global, x_local)\n",
        "            x_global, x_local = x\n",
        "\n",
        "            # Global stream: normalized then passed through attention (global / prefix semantics)\n",
        "            y_global = self.ln_global(x_global)\n",
        "            att_out  = self.att(y_global, use_cache=use_cache, use_swa=use_swa)\n",
        "\n",
        "            # Local stream: normalized then passed through MoE/FFN (local token-wise expertise)\n",
        "            y_local = self.ln_local(x_local)\n",
        "            ff_out  = self.ff(y_local)\n",
        "\n",
        "            # Residual updates are kept separate for the two streams\n",
        "            enhanced_att = self.enhance_global_ctx(att_out)\n",
        "            x_global = x_global + self.drop_shortcut(enhanced_att)\n",
        "            x_local  = x_local  + self.drop_shortcut(ff_out)\n",
        "\n",
        "            return (x_global, x_local)\n",
        "\n",
        "        if self.use_parallel_att:\n",
        "            # Parallel residual: x + (att(ln(x)) + ff(ln(x))) / âˆš2\n",
        "            y = self.ln(x)\n",
        "            att_out = self.att(y, use_cache=use_cache, use_swa=use_swa)\n",
        "            ff_out  = self.ff(y)\n",
        "            z = att_out + ff_out\n",
        "            z = self.res_scale * z\n",
        "            x = x + self.drop_shortcut(z)\n",
        "            return x\n",
        "        else:\n",
        "            # Classic: x + Att(ln1(x)) then x + FF(ln2(x))\n",
        "            y = self.ln1(x)\n",
        "            x = x + self.drop_shortcut(self.att(y, use_cache=use_cache, use_swa=use_swa))\n",
        "\n",
        "            y2 = self.ln2(x)\n",
        "            x = x + self.drop_shortcut(self.ff(y2))\n",
        "\n",
        "            return x\n",
        "\n",
        "\n",
        "class MyGPT_GQA_SWA(nn.Module):\n",
        "    \"\"\"\n",
        "    Top-level Transformer orchestrator implementing a decoupled Dual-Stream\n",
        "    architecture with Grouped-Query Attention (GQA) and Sliding Window Attention (SWA).\n",
        "\n",
        "    Key Features:\n",
        "    - Dual-Stream Logic: Separates global context from local processing.\n",
        "    - Inference Optimization: Integrated KV-cache management for O(1) decoding.\n",
        "    - MODERN Components: Support for RMSNorm, SwiGLU, and Flash-based attention.\n",
        "    - Multimodal Injection: Supports CLAP audio features via a linear projection.\n",
        "    \"\"\"\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "\n",
        "        self.use_RMSNorm = cfg.get(\"use_RMSNorm\", False)\n",
        "        # NEW: propagate dual-stream flag to model level\n",
        "        self.use_dual_stream = cfg.get(\"use_dual_stream\", False)\n",
        "\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        # NEW: Audio Projection Layer\n",
        "        # Maps CLAP 1024 features to the model's internal embedding dimension (e.g., 1024)\n",
        "        # self.audio_proj = nn.Linear(1024, cfg[\"emb_dim\"])\n",
        "        self.audio_proj = SwiGLU_FFN(1024);\n",
        "        # Use ModuleList so we can pass use_cache through each block\n",
        "        self.trm_blocks = nn.ModuleList(\n",
        "            [TransformerBlock_GQA_SWA(cfg) for _ in range(cfg[\"n_layers\"])]\n",
        "        )\n",
        "\n",
        "        # --- Norms depend on mode ---\n",
        "        Normalization = RMSNorm if self.use_RMSNorm else NormLayer\n",
        "        self.final_norm = Normalization(cfg[\"emb_dim\"])\n",
        "\n",
        "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
        "\n",
        "        # Track current position in cached sequence for inference\n",
        "        # (how many tokens have been seen so far in cache mode)\n",
        "        self.cache_pos = 0\n",
        "\n",
        "        # For final combination of global/local streams (when use_dual_stream=True)\n",
        "        self._dual_res_scale = 1.0 / math.sqrt(2.0)\n",
        "\n",
        "    def reset_cache(self):\n",
        "        \"\"\"Resets the KV cache in all attention modules within the transformer blocks.\"\"\"\n",
        "        for block in self.trm_blocks:\n",
        "            block.att.reset_cache()\n",
        "        # Also reset our positional counter\n",
        "        self.cache_pos = 0\n",
        "\n",
        "    def forward(self, x, audio_features=None, use_cache: bool = False, use_swa: bool = False):\n",
        "        batch_size, seq_len = x.shape\n",
        "        device = x.device\n",
        "\n",
        "        # 1. POSITIONAL LOGIC\n",
        "        if use_cache:\n",
        "            # In cache mode, we may be doing:\n",
        "            #   - prefill: first call after reset_cache (seq_len = T_prompt)\n",
        "            #   - incremental decode: subsequent calls (seq_len = 1)\n",
        "            assert self.cache_pos + seq_len <= self.pos_emb.num_embeddings, (\n",
        "                f\"Total length {self.cache_pos + seq_len} exceeds context length \"\n",
        "                f\"{self.pos_emb.num_embeddings}\"\n",
        "            )\n",
        "\n",
        "            pos_start = self.cache_pos\n",
        "            pos_ids = torch.arange(pos_start, pos_start + seq_len, device=device)\n",
        "            self.cache_pos += seq_len\n",
        "        else:\n",
        "            # Training / no cache: positions always start at 0\n",
        "            assert seq_len <= self.pos_emb.num_embeddings, \\\n",
        "                f\"Sequence length {seq_len} exceeds context length {self.pos_emb.num_embeddings}\"\n",
        "            pos_ids = torch.arange(seq_len, device=device)\n",
        "\n",
        "        # 2. EMBEDDING FUSION\n",
        "        # Get standard text embeddings\n",
        "        x = self.tok_emb(x) + self.pos_emb(pos_ids)\n",
        "\n",
        "        # NEW: Multimodal Injection Logic\n",
        "        if audio_features is not None:\n",
        "            # Project CLAP features to embedding dimension [Batch, 512] -> [Batch, 1, Emb_dim]\n",
        "            audio_x = self.audio_proj(audio_features).unsqueeze(1)\n",
        "\n",
        "            # CONCAT: Audio becomes the 'prefix' token (Token 0)\n",
        "            # This allows all text tokens to attend to the audio context via GQA\n",
        "            x = torch.cat([audio_x, x], dim=1)\n",
        "\n",
        "        x = self.drop_emb(x)\n",
        "\n",
        "        # 3. DUAL-STREAM PROCESSING\n",
        "        if self.use_dual_stream:\n",
        "            # NEW: dual global/local streams\n",
        "            #   - x_global: updated only by attention (global prefix semantics)\n",
        "            #   - x_local:  updated only by MoE/FFN (local token-wise expertise)\n",
        "            x_global = x\n",
        "            x_local  = x\n",
        "\n",
        "            state = (x_global, x_local)\n",
        "            # Pass use_cache through every TransformerBlockGQA\n",
        "            for block in self.trm_blocks:\n",
        "                state = block(state, use_cache=use_cache, use_swa=use_swa)\n",
        "\n",
        "            x_global, x_local = state\n",
        "\n",
        "            # Only now combine global + local once, before final norm + classifier\n",
        "            x = (x_global + x_local) * self._dual_res_scale\n",
        "        else:\n",
        "            # Pass use_cache through every TransformerBlockGQA\n",
        "            for block in self.trm_blocks:\n",
        "                x = block(x, use_cache=use_cache, use_swa=use_swa)\n",
        "\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "def load_standard_baseline(model, sd_hf):\n",
        "    \"\"\"\n",
        "    Multimodal-aware surgical loader.\n",
        "    Transplants GPT-2 intelligence while leaving the Audio Bridge for training.\n",
        "    \"\"\"\n",
        "    print(\"ðŸ©¹ Initializing Multimodal-Aware Baseline weight mapping...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # 1. GLOBAL EMBEDDINGS\n",
        "        # GPT-2 Medium uses 50257. If your model uses more (special tokens),\n",
        "        # we only copy the overlapping pre-trained weights.\n",
        "        hf_wte = sd_hf['wte.weight']\n",
        "        model.tok_emb.weight[:hf_wte.size(0)].copy_(hf_wte)\n",
        "        model.pos_emb.weight.copy_(sd_hf['wpe.weight'])\n",
        "\n",
        "        # 2. TRANSFORMER BLOCKS (The \"Brain\")\n",
        "        for i, block in enumerate(model.trm_blocks):\n",
        "            prefix = f'h.{i}.'\n",
        "\n",
        "            # Normalization (NormLayer: gamma/bias)\n",
        "            block.ln1.gamma.copy_(sd_hf[f'{prefix}ln_1.weight'])\n",
        "            block.ln1.bias.copy_(sd_hf[f'{prefix}ln_1.bias'])\n",
        "            block.ln2.gamma.copy_(sd_hf[f'{prefix}ln_2.weight'])\n",
        "            block.ln2.bias.copy_(sd_hf[f'{prefix}ln_2.bias'])\n",
        "\n",
        "            # Attention (GQA/SWA mapping)\n",
        "            qkv_w = sd_hf[f'{prefix}attn.c_attn.weight'].t()\n",
        "            qkv_b = sd_hf[f'{prefix}attn.c_attn.bias']\n",
        "            qw, kw, vw = qkv_w.chunk(3, dim=0)\n",
        "            qb, kb, vb = qkv_b.chunk(3, dim=0)\n",
        "\n",
        "            block.att.W_q.weight.copy_(qw); block.att.W_q.bias.copy_(qb)\n",
        "            block.att.W_k.weight.copy_(kw); block.att.W_k.bias.copy_(kb)\n",
        "            block.att.W_v.weight.copy_(vw); block.att.W_v.bias.copy_(vb)\n",
        "\n",
        "            block.att.final_proj.weight.copy_(sd_hf[f'{prefix}attn.c_proj.weight'].t())\n",
        "            block.att.final_proj.bias.copy_(sd_hf[f'{prefix}attn.c_proj.bias'])\n",
        "\n",
        "            # Feed-Forward\n",
        "            block.ff.layers[0].weight.copy_(sd_hf[f'{prefix}mlp.c_fc.weight'].t())\n",
        "            block.ff.layers[0].bias.copy_(sd_hf[f'{prefix}mlp.c_fc.bias'])\n",
        "            block.ff.layers[2].weight.copy_(sd_hf[f'{prefix}mlp.c_proj.weight'].t())\n",
        "            block.ff.layers[2].bias.copy_(sd_hf[f'{prefix}mlp.c_proj.bias'])\n",
        "\n",
        "        # 3. FINAL HEAD & NORM\n",
        "        model.final_norm.gamma.copy_(sd_hf['ln_f.weight'])\n",
        "        model.final_norm.bias.copy_(sd_hf['ln_f.bias'])\n",
        "\n",
        "        # Tie weights for the head if using standard vocabulary\n",
        "        # If model.out_head is bigger than hf_wte, we only copy the intersection\n",
        "        model.out_head.weight[:hf_wte.size(0)].copy_(hf_wte)\n",
        "\n",
        "        # 4. THE AUDIO BRIDGE (Crucial!)\n",
        "        # We do NOT load weights for model.audio_proj here.\n",
        "        # It remains randomly initialized so it can learn during your 14-hour marathon.\n",
        "        print(\"â„¹ï¸  Note: 'audio_proj' remains randomly initialized for training.\")\n",
        "\n",
        "    print(\"âœ… Multimodal Baseline loaded. The 'Brain' is pre-trained, the 'Ears' are ready to learn.\")"
      ],
      "metadata": {
        "id": "t-ZLc3ULc_jS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_token_ids(text, tokenizer):\n",
        "    \"\"\"Encodes text while explicitly allowing the <|endoftext|> stop signal.\"\"\"\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    return torch.tensor(encoded).unsqueeze(0)\n",
        "\n",
        "def top_k_top_p_filtering(logits, top_k=None, top_p=0.9):\n",
        "    \"\"\"\n",
        "    Corrected filtering that avoids the 'Index tensor' dimensionality error.\n",
        "    \"\"\"\n",
        "    B, V = logits.shape\n",
        "\n",
        "    # 1. Top-K Filtering\n",
        "    if top_k is not None and 0 < top_k < V:\n",
        "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "        logits.masked_fill_(indices_to_remove, float('-inf'))\n",
        "\n",
        "    # 2. Top-P (Nucleus) Filtering\n",
        "    if top_p is not None and 0.0 < top_p < 1.0:\n",
        "        # Sort logits in descending order\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True, dim=-1)\n",
        "\n",
        "        # Calculate cumulative probabilities\n",
        "        sorted_probs = F.softmax(sorted_logits, dim=-1)\n",
        "        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "\n",
        "        # Create the mask on the SORTED tensor (stays 2D)\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "\n",
        "        # Shift mask to ensure we keep at least the first token above threshold\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = False\n",
        "\n",
        "        # Apply the mask to the sorted logits\n",
        "        sorted_logits.masked_fill_(sorted_indices_to_remove, float('-inf'))\n",
        "\n",
        "        # RE-SCATTER: Put the masked values back into their original vocabulary positions\n",
        "        # This ensures the dimensionality [B, V] is preserved perfectly\n",
        "        logits = torch.full_like(logits, float('-inf')).scatter_(dim=-1, index=sorted_indices, src=sorted_logits)\n",
        "\n",
        "    return logits\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_multimodal(\n",
        "  model, idx, audio_features, max_new_tokens, context_size,\n",
        "  use_cache=True, temperature=0.2, top_k=None, top_p=0.9,\n",
        "  repetition_penalty=1.1,\n",
        "  eos_id=50256\n",
        "):\n",
        "  model.eval()\n",
        "  if use_cache:\n",
        "      model.reset_cache()\n",
        "\n",
        "  generated_tokens = [] # Tracks history to apply penalty\n",
        "\n",
        "  for step in range(max_new_tokens):\n",
        "      # 1. KV-Cache Optimization:\n",
        "      # Inject audio ONLY on Step 0. Afterwards, it's stored in memory.\n",
        "      if use_cache:\n",
        "          if step == 0:\n",
        "              idx_cond = idx[:, -context_size:]\n",
        "              audio_cond = audio_features\n",
        "          else:\n",
        "              idx_cond = idx[:, -1:] # Only process the very last word\n",
        "              audio_cond = None      # Audio is already cached\n",
        "      else:\n",
        "          idx_cond = idx[:, -context_size:]\n",
        "          audio_cond = audio_features\n",
        "\n",
        "      # 2. Forward Pass\n",
        "      logits = model(idx_cond, audio_features=audio_cond, use_cache=use_cache)[:, -1, :]\n",
        "\n",
        "      # 3. Apply Repetition Penalty (The \"Loop Killer\")\n",
        "      for token in set(generated_tokens):\n",
        "          if logits[0, token] > 0:\n",
        "              logits[0, token] /= repetition_penalty\n",
        "          else:\n",
        "              logits[0, token] *= repetition_penalty\n",
        "\n",
        "      # 4. Sampling\n",
        "      if temperature <= 0.0:\n",
        "          next_token_id = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "      else:\n",
        "          logits = logits / temperature\n",
        "          logits = top_k_top_p_filtering(logits, top_k=top_k, top_p=top_p)\n",
        "          probs = torch.softmax(logits, dim=-1)\n",
        "          next_token_id = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "      # 5. Append and Check Stop Signal\n",
        "      generated_tokens.append(next_token_id.item())\n",
        "      idx = torch.cat((idx, next_token_id), dim=-1)\n",
        "\n",
        "      if next_token_id.item() == eos_id:\n",
        "          break\n",
        "\n",
        "  return idx\n",
        "\n",
        "def generate_multimodal_sample(model, tokenizer, device, question, audio_key, features_dict):\n",
        "    model.eval()\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "\n",
        "    # Format matches your JSON training data exactly\n",
        "    prompt = (\n",
        "        f\"Below is an instruction describing an audio task. \"\n",
        "        f\"Respond appropriately using the provided audio input.\\n\\n\"\n",
        "        f\"### Instruction:\\n{question}\\n\\n\"\n",
        "        f\"### Response:\\n\"\n",
        "    )\n",
        "\n",
        "    encoded = text_to_token_ids(prompt, tokenizer).to(device)\n",
        "    audio_vector = features_dict[audio_key].to(device)\n",
        "\n",
        "    # Ensure we use the correct ID for Tiktoken\n",
        "    tiktoken_eos_id = tokenizer.encode(\"<|endoftext|>\", allowed_special={'<|endoftext|>'})[0]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        token_ids = generate_multimodal(\n",
        "            model=model,\n",
        "            idx=encoded,\n",
        "            audio_features=audio_vector,\n",
        "            max_new_tokens=200,\n",
        "            context_size=context_size,\n",
        "            temperature=0.2,         # Adds variety\n",
        "            top_p=0.9,               # Logical filtering\n",
        "            repetition_penalty=1.1,  # Force the model to speak English\n",
        "            eos_id=tiktoken_eos_id\n",
        "        )\n",
        "\n",
        "    decoded_text = tokenizer.decode(token_ids[0].tolist())\n",
        "    print(f\"\\n--- INFERENCE RESULT ({audio_key}) ---\\n\")\n",
        "    print(decoded_text)\n",
        "    model.train()"
      ],
      "metadata": {
        "id": "WhOTVFJJcRuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AudioAnalyzerAssistant:\n",
        "    \"\"\"\n",
        "    The High-Level Inference Manager for the Auditory Reasoning System.\n",
        "\n",
        "    Key Responsibilities:\n",
        "    - Hardware Abstraction: Dynamically detects and orchestrates inference on available GPU accelerators (CUDA) or seamlessly falls back to CPU.\n",
        "    - Multimodal Bridge: Manages the 'Ears' (CLAP) and 'Brain' (Custom GPT-2) to ensure synchronized feature injection.\n",
        "    - Logic Enforcer: Applies the 'Auditory Analyst' system prompt to force rigorous Chain-of-Thought generation.\n",
        "    - System Extensibility: Acts as the primary auditory reasoning module within the larger Visual Analyzer Assistant framework.\n",
        "    \"\"\"\n",
        "    def __init__(self, model_path, tokenizer, CHOSEN_MODEL=\"gpt2-medium (355M)\"):\n",
        "        # 1. Device selection (Auto-detect GPU or CPU)\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # 2. EXACT CLAP from your training (The \"Ears\")\n",
        "        # Ensure 'version' matches what you used in the data script\n",
        "        self.clap_model = CLAP(version='2023', use_cuda=torch.cuda.is_available())\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        # 3. Backbone Setup (The \"Brain\")\n",
        "        gpt_cfg = BASE_CONFIG.copy()\n",
        "        gpt_cfg.update(model_configs[CHOSEN_MODEL])\n",
        "        # Force these to False to match the 'Surgical Loader' logic\n",
        "        gpt_cfg.update({\n",
        "            \"use_dual_stream\": False,\n",
        "            \"use_adaptive_moe\": False,\n",
        "            \"use_parallel_att\": False,\n",
        "            \"kv_groups\": gpt_cfg[\"n_heads\"],\n",
        "            \"drop_rate\": 0.0 # CRITICAL: No dropout during inference\n",
        "        })\n",
        "        self.gpt_model = MyGPT_GQA_SWA(gpt_cfg).to(self.device)\n",
        "\n",
        "        # 4. Load Milestone Weights\n",
        "        print(f\"ðŸ”„ Loading weights from: {os.path.basename(model_path)}...\")\n",
        "        checkpoint = torch.load(model_path, map_location='cpu')\n",
        "        self.gpt_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.gpt_model.to(self.device)\n",
        "        self.gpt_model.eval() # CRITICAL: Sets model to evaluation mode\n",
        "        print(f\"âœ… Assistant Ready on {self.device}\")\n",
        "\n",
        "    def analyze_audio(self, task_list):\n",
        "        context_size = self.gpt_model.pos_emb.weight.shape[0]\n",
        "        # Ensure we have the correct End-of-String ID for stopping\n",
        "        tiktoken_eos_id = self.tokenizer.encode(\"<|endoftext|>\", allowed_special={'<|endoftext|>'})[0]\n",
        "\n",
        "        final_report = []\n",
        "\n",
        "        for task in task_list:\n",
        "            audio_path = task.get(\"path\")\n",
        "            questions = task.get(\"questions\", [])\n",
        "\n",
        "            if not audio_path or not os.path.exists(audio_path):\n",
        "                print(f\"âš ï¸ File not found: {audio_path}\")\n",
        "                continue\n",
        "\n",
        "            # --- STEP 1: HEAR THE AUDIO (Feature Extraction) ---\n",
        "            try:\n",
        "                with torch.no_grad():\n",
        "                    # Get the 1024-dim CLAP embedding\n",
        "                    audio_emb = self.clap_model.get_audio_embeddings([audio_path]).to(self.device)\n",
        "            except Exception as e:\n",
        "                print(f\"âŒ CLAP Error on {audio_path}: {e}\")\n",
        "                continue\n",
        "\n",
        "            print(f\"\\n--- ðŸŽ§ Analyzing: {os.path.basename(audio_path)} ---\")\n",
        "\n",
        "            # --- STEP 2: REASONING LOOP ---\n",
        "            for q in questions:\n",
        "                # OPTIMIZED PROMPT: Matches your training data structure exactly\n",
        "                # We pre-fill \"<|start_thought|>\" to trigger the reasoning mode immediately.\n",
        "                prompt = (\n",
        "                    f\"Below is an instruction describing an audio task. \"\n",
        "                    f\"Respond appropriately using the provided audio input.\\n\\n\"\n",
        "                    f\"### Instruction:\\n{q}\\n\\n\"\n",
        "                    f\"### Response:\\n<|start_thought|>\"\n",
        "                )\n",
        "\n",
        "                encoded = text_to_token_ids(prompt, self.tokenizer).to(self.device)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    output_ids = generate_multimodal(\n",
        "                        model = self.gpt_model,\n",
        "                        idx = encoded,\n",
        "                        audio_features=audio_emb,\n",
        "                        max_new_tokens=256,   # Enough room for the full thought process\n",
        "                        temperature=0.1,      # Low temp = High precision (Fact-based)\n",
        "                        top_p=0.9,\n",
        "                        repetition_penalty=1.1, # Gentle penalty to keep it moving\n",
        "                        eos_id=tiktoken_eos_id,\n",
        "                        context_size = context_size\n",
        "                    )\n",
        "\n",
        "                # Decode the raw tokens\n",
        "                response = self.tokenizer.decode(output_ids[0].tolist())\n",
        "\n",
        "                # --- CLEANING: Extract just the reasoning and answer ---\n",
        "                # 1. Remove the prompt\n",
        "                raw_output = response.split(\"### Response:\\n\")[-1]\n",
        "\n",
        "                # 2. Clean up the tags for display\n",
        "                # We keep the text but remove the technical tags for the user report\n",
        "                clean_res = raw_output.replace(\"<|start_thought|>\", \"\").replace(\"<|end_thought|>\", \"\\nâž¡ï¸ Answer:\").strip()\n",
        "\n",
        "                # 3. Cut off at the end signal\n",
        "                if \"<|endoftext|>\" in clean_res:\n",
        "                    clean_res = clean_res.split(\"<|endoftext|>\")[0].strip()\n",
        "\n",
        "                print(f\"â“ Q: {q}\")\n",
        "                print(f\"ðŸ¤– Logic Trace: {clean_res[:150]}...\") # Preview first 150 chars\n",
        "                print(f\"   (Full logic saved to report)\\n\")\n",
        "\n",
        "                final_report.append({\n",
        "                    \"Audio\": os.path.basename(audio_path),\n",
        "                    \"Question\": q,\n",
        "                    \"AI Reasoning\": clean_res # Saves the full explanation\n",
        "                })\n",
        "\n",
        "        return final_report"
      ],
      "metadata": {
        "id": "BVtbWObFd5aB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_CONFIG = {\n",
        "    # Text Processing\n",
        "    \"vocab_size\": 50257,        # Total 'words' the model knows (Standard GPT-2)\n",
        "    \"context_length\": 1024,     # Maximum tokens the model can \"remember\" at once\n",
        "\n",
        "    # Layer Stability\n",
        "    \"qkv_bias\": True,           # Adds learnable bias to the Attention projections\n",
        "    \"drop_rate\": 0.0,           # Regularization: % of neurons to disable during training\n",
        "    \"use_RMSNorm\": False,       # Toggle: Standard LayerNorm vs faster Llama-style Norm\n",
        "\n",
        "    # Research Architecture Switches\n",
        "    \"use_dual_stream\": False,   # Toggle: Separates Global context from Local expertise\n",
        "    \"use_adaptive_moe\": False,  # Toggle: Enables the Hierarchical Mixture of Experts\n",
        "    \"use_parallel_att\": False,  # Toggle: Runs Attention and FFN at the same time (PaLM style)\n",
        "\n",
        "    # GQA (Grouped Query Attention)\n",
        "    #\n",
        "    \"kv_groups\": 16,            # Number of KV heads (saves VRAM compared to standard MHA)\n",
        "\n",
        "    # MoE (Mixture of Experts) Settings\n",
        "    #\n",
        "    \"e_num\": 1,                 # Total number of available expert brains\n",
        "    \"moe_groups\": 1,            # How we cluster experts into 'knowledge neighborhoods'\n",
        "    \"moe_group_top_p\": 0.7,     # Adaptive threshold for selecting active groups\n",
        "    \"moe_top_p\": 0.9,           # Adaptive threshold for selecting experts within groups\n",
        "    \"moe_max_groups\": 1,        # Limit on how many groups a token can visit\n",
        "    \"moe_max_k\": 1,             # Limit on how many experts per group a token can use\n",
        "\n",
        "    # SWA (Sliding Window Attention)\n",
        "    #\n",
        "    \"window_size\": 1024,        # Cache size for inference (ring buffer limit)\n",
        "    \"swa_size\": 1024,           # Look-back limit during training (saves computation)\n",
        "}\n",
        "\n",
        "# Standard OpenAI Scaling blue-prints\n",
        "model_configs = {\n",
        "    \"gpt2-small (124M)\":  {\"emb_dim\": 768,  \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large (774M)\":  {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl (1558M)\":    {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}\n",
        "\n",
        "# Linking our readable names to the official Hugging Face Hub identifiers\n",
        "mapping = {\n",
        "    \"gpt2-small (124M)\":  \"gpt2\",\n",
        "    \"gpt2-medium (355M)\": \"gpt2-medium\",\n",
        "    \"gpt2-large (774M)\":  \"gpt2-large\",\n",
        "    \"gpt2-xl (1558M)\":    \"gpt2-xl\"\n",
        "}"
      ],
      "metadata": {
        "id": "3py8V5xMeZfZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}